#! /usr/bin/env python
# -*- coding: utf-8 -*-
# Authors: SDG DS unit
"""
This module provides a function to interact with OpenAI's models, sending a 
prompt and receiving generated content. The function `call_llm` allows you 
to specify parameters like the model, temperature, and maximum tokens for the 
generation.

Function:
    call_llm: Sends a prompt to an OpenAI model and returns the generated content.

Dependencies:
    - openai: The OpenAI API client used to interact with the models.
"""

import openai


def call_llm(
        prompt: str,
        model: str = "gpt-4o-mini",
        temperature: float = 0.4,
        max_tokens: int = 15000,
) -> str:
    """
    Calls the OpenAI model with the given prompt and returns the generated content.
    Allows configuration of parameters such as the model, temperature, and tokens.

    Args:
        prompt (str): The prompt to send to the LLM model.
        model (str): The OpenAI model to use (default is "gpt-4o-mini").
        temperature (float): The temperature parameter for generation (default is 0.4).
        max_tokens (int): The maximum number of tokens to generate (default is 15000).

    Returns:
        str: The content generated by the model.
    """
    messages = [{"role": "user", "content": prompt}]
    client = openai.Client()

    # Make the request to the model
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
    )

    # Process the response
    response_content = completion.choices[0].message.content.strip()

    return response_content

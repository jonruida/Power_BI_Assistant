#! /usr/bin/env python
# -*- coding: utf-8 -*-
# Authors: SDG DS unit
"""
This module provides a function to interact with OpenAI's models, sending a 
prompt and receiving generated content. The function `call_llm` allows you 
to specify parameters like the model, temperature, and maximum tokens for the 
generation.

Function:
    call_llm: Sends a prompt to an OpenAI model and returns the generated content.

Dependencies:
    - Groq API: The Groq API client used to interact with the models.
"""

from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("GROQ_API_KEY")

def call_llm(
        prompt: str,
        model: str = "llama-3.3-70b-specdec",
        temperature: float = 0.4,
        max_tokens: int = 15000,
) -> str:
    """
    Calls the OpenAI model with the given prompt and returns the generated content.
    Allows configuration of parameters such as the model, temperature, and tokens.

    Args:
        prompt (str): The prompt to send to the LLM model.
        model (str): The llama3 model to use (default is "llama-3.3-70b-specdec").
        temperature (float): The temperature parameter for generation (default is 0.4).
        max_tokens (int): The maximum number of tokens to generate (default is 15000).

    Returns:
        str: The content generated by the model.
    """
    messages = [
        ("system", "You are a helpful assistant."),
        ("human", prompt)
    ]
    # Inicializa el cliente ChatGroq con el modelo y la clave de API
    chat_groq = ChatGroq(
        model_name="llama-3.3-70b-specdec",
        groq_api_key=api_key,
        temperature=temperature
    )

    # Usa invoke para hacer la solicitud al modelo
    response = chat_groq.invoke(messages)

    # Procesa la respuesta
    response_content =  response.content.strip()

    return response_content
